2021-10-17 09:53:31 - INFO - 8 - Saving results to: results/gnmt
2021-10-17 09:53:31 - INFO - 8 - Run arguments: Namespace(Gpipe=False, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/gf3/home/xyye/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=False, epochs=8, eval=True, grad_clip=5.0, hidden_size=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, lr=0.001, math='fp32', max_length_test=150, max_length_train=50, max_length_val=125, max_size=None, min_length_test=0, min_length_train=0, min_length_val=0, num_buckets=5, num_layers=8, num_split=4, optimizer='Adam', optimizer_extra='{}', partition=4, print_freq=10, rank=8, remain_steps=0.666, results_dir='results', resume=None, save='gnmt', save_all=False, save_freq=5000, save_path='results/gnmt', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=16, test_loader_workers=0, train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_num=4, val_batch_size=16, val_loader_workers=0, warmup_steps=200)
2021-10-17 09:53:32 - INFO - 8 - Worker 8 is using worker seed: 3348590967
2021-10-17 09:53:32 - INFO - 8 - Building vocabulary from /gf3/home/xyye/wmt16_de_en/vocab.bpe.32000
2021-10-17 09:53:32 - INFO - 8 - Size of vocabulary: 32317
2021-10-17 09:53:32 - INFO - 8 - Processing data from /gf3/home/xyye/wmt16_de_en/train.tok.clean.bpe.32000.en
2021-10-17 09:53:34 - INFO - 8 - Processing data from /gf3/home/xyye/wmt16_de_en/train.tok.clean.bpe.32000.de
2021-10-17 09:53:36 - INFO - 8 - Filtering data, min len: 0, max len: 50
2021-10-17 09:53:43 - INFO - 8 - Pairs before: 4068191, after: 3498161
2021-10-17 09:53:44 - INFO - 8 - Processing data from /gf3/home/xyye/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2021-10-17 09:53:44 - INFO - 8 - Processing data from /gf3/home/xyye/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2021-10-17 09:53:44 - INFO - 8 - Filtering data, min len: 0, max len: 125
2021-10-17 09:53:44 - INFO - 8 - Pairs before: 5100, after: 5100
2021-10-17 09:53:44 - INFO - 8 - Processing data from /gf3/home/xyye/wmt16_de_en/newstest2014.tok.bpe.32000.en
2021-10-17 09:53:44 - INFO - 8 - Filtering data, min len: 0, max len: 150
2021-10-17 09:53:44 - INFO - 8 - Pairs before: 3003, after: 3003
2021-10-17 09:53:46 - INFO - 8 - Sequential(
  (Emb1): embed1(
    (embed): Embedding(32320, 1024, padding_idx=0)
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm1): lstm1(
    (layer): LSTM(1024, 1024, bidirectional=True)
  )
  (Dropout1): dropout1(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm2): lstm1(
    (layer): LSTM(2048, 1024)
  )
  (Dropout2): dropout1(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm3): lstm1(
    (layer): LSTM(1024, 1024)
  )
  (Dropout3): dropout1(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm4): lstm1(
    (layer): LSTM(1024, 1024)
  )
)
2021-10-17 09:53:46 - INFO - 8 - Training optimizer config: {'optimizer': 'Adam', 'lr': 0.001}
2021-10-17 09:53:46 - INFO - 8 - Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2021-10-17 09:53:46 - INFO - 8 - Number of stage0 parameters: 79273984
2021-10-17 09:53:51 - INFO - 8 - Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
2021-10-17 09:53:51 - INFO - 8 - Scheduler warmup steps: 200
2021-10-17 09:53:51 - INFO - 8 - Scheduler remain steps: 9078
2021-10-17 09:53:51 - INFO - 8 - Scheduler decay interval: 1138
2021-10-17 09:53:51 - INFO - 8 - Scheduler decay factor: 0.5
2021-10-17 09:53:51 - INFO - 8 - Scheduler max decay steps: 4
2021-10-17 09:53:51 - INFO - 8 - Starting epoch 0
2021-10-17 09:53:53 - INFO - 8 - Sampler for epoch 0 uses seed 3210064540
2021-10-17 09:53:54 - INFO - 8 - TRAIN [0][0/1704]	Time 1.604 (0.000)	Data 5.38e-01 (0.00e+00)	Tok/s 19817 (0)	LR 1.023e-05
2021-10-17 09:54:01 - INFO - 8 - TRAIN [0][10/1704]	Time 0.586 (0.740)	Data 3.91e-04 (4.24e-04)	Tok/s 38857 (37777)	LR 1.288e-05
2021-10-17 09:54:08 - INFO - 8 - TRAIN [0][20/1704]	Time 0.581 (0.706)	Data 3.86e-04 (3.98e-04)	Tok/s 39115 (37603)	LR 1.622e-05
2021-10-17 09:54:15 - INFO - 8 - TRAIN [0][30/1704]	Time 0.647 (0.707)	Data 3.46e-04 (3.92e-04)	Tok/s 35473 (37141)	LR 2.042e-05
2021-10-17 09:54:22 - INFO - 8 - TRAIN [0][40/1704]	Time 0.748 (0.690)	Data 4.25e-04 (3.96e-04)	Tok/s 42417 (37333)	LR 2.570e-05
2021-10-17 09:54:28 - INFO - 8 - TRAIN [0][50/1704]	Time 0.782 (0.679)	Data 4.18e-04 (3.89e-04)	Tok/s 40621 (37183)	LR 3.236e-05
2021-10-17 09:54:35 - INFO - 8 - TRAIN [0][60/1704]	Time 0.782 (0.681)	Data 3.80e-04 (3.84e-04)	Tok/s 40468 (37172)	LR 4.074e-05
2021-10-17 09:54:42 - INFO - 8 - TRAIN [0][70/1704]	Time 0.761 (0.679)	Data 3.26e-04 (3.88e-04)	Tok/s 41748 (36980)	LR 5.129e-05
2021-10-17 09:54:48 - INFO - 8 - TRAIN [0][80/1704]	Time 0.612 (0.680)	Data 3.56e-04 (3.85e-04)	Tok/s 36802 (37032)	LR 6.457e-05
