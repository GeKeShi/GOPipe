2021-10-17 09:53:31 - INFO - 12 - Saving results to: results/gnmt
2021-10-17 09:53:31 - INFO - 12 - Run arguments: Namespace(Gpipe=False, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/gf3/home/xyye/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=False, epochs=8, eval=True, grad_clip=5.0, hidden_size=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, lr=0.001, math='fp32', max_length_test=150, max_length_train=50, max_length_val=125, max_size=None, min_length_test=0, min_length_train=0, min_length_val=0, num_buckets=5, num_layers=8, num_split=4, optimizer='Adam', optimizer_extra='{}', partition=4, print_freq=10, rank=12, remain_steps=0.666, results_dir='results', resume=None, save='gnmt', save_all=False, save_freq=5000, save_path='results/gnmt', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=16, test_loader_workers=0, train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_num=4, val_batch_size=16, val_loader_workers=0, warmup_steps=200)
2021-10-17 09:53:32 - INFO - 12 - Worker 12 is using worker seed: 3348590967
2021-10-17 09:53:32 - INFO - 12 - Building vocabulary from /gf3/home/xyye/wmt16_de_en/vocab.bpe.32000
2021-10-17 09:53:32 - INFO - 12 - Size of vocabulary: 32317
2021-10-17 09:53:32 - INFO - 12 - Processing data from /gf3/home/xyye/wmt16_de_en/train.tok.clean.bpe.32000.en
2021-10-17 09:53:33 - INFO - 12 - Processing data from /gf3/home/xyye/wmt16_de_en/train.tok.clean.bpe.32000.de
2021-10-17 09:53:36 - INFO - 12 - Filtering data, min len: 0, max len: 50
2021-10-17 09:53:42 - INFO - 12 - Pairs before: 4068191, after: 3498161
2021-10-17 09:53:42 - INFO - 12 - Processing data from /gf3/home/xyye/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2021-10-17 09:53:43 - INFO - 12 - Processing data from /gf3/home/xyye/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2021-10-17 09:53:43 - INFO - 12 - Filtering data, min len: 0, max len: 125
2021-10-17 09:53:43 - INFO - 12 - Pairs before: 5100, after: 5100
2021-10-17 09:53:43 - INFO - 12 - Processing data from /gf3/home/xyye/wmt16_de_en/newstest2014.tok.bpe.32000.en
2021-10-17 09:53:43 - INFO - 12 - Filtering data, min len: 0, max len: 150
2021-10-17 09:53:43 - INFO - 12 - Pairs before: 3003, after: 3003
2021-10-17 09:53:46 - INFO - 12 - Sequential(
  (Emb1): embed1(
    (embed): Embedding(32320, 1024, padding_idx=0)
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm1): lstm1(
    (layer): LSTM(1024, 1024, bidirectional=True)
  )
  (Dropout1): dropout1(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm2): lstm1(
    (layer): LSTM(2048, 1024)
  )
  (Dropout2): dropout1(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm3): lstm1(
    (layer): LSTM(1024, 1024)
  )
  (Dropout3): dropout1(
    (dropout): Dropout(p=0.2, inplace=False)
  )
  (E_lstm4): lstm1(
    (layer): LSTM(1024, 1024)
  )
)
2021-10-17 09:53:46 - INFO - 12 - Training optimizer config: {'optimizer': 'Adam', 'lr': 0.001}
2021-10-17 09:53:46 - INFO - 12 - Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2021-10-17 09:53:46 - INFO - 12 - Number of stage0 parameters: 79273984
2021-10-17 09:53:51 - INFO - 12 - Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
2021-10-17 09:53:51 - INFO - 12 - Scheduler warmup steps: 200
2021-10-17 09:53:51 - INFO - 12 - Scheduler remain steps: 9078
2021-10-17 09:53:51 - INFO - 12 - Scheduler decay interval: 1138
2021-10-17 09:53:51 - INFO - 12 - Scheduler decay factor: 0.5
2021-10-17 09:53:51 - INFO - 12 - Scheduler max decay steps: 4
2021-10-17 09:53:51 - INFO - 12 - Starting epoch 0
2021-10-17 09:53:53 - INFO - 12 - Sampler for epoch 0 uses seed 3210064540
2021-10-17 09:53:54 - INFO - 12 - TRAIN [0][0/1704]	Time 1.521 (0.000)	Data 6.19e-01 (0.00e+00)	Tok/s 20936 (0)	LR 1.023e-05
2021-10-17 09:54:01 - INFO - 12 - TRAIN [0][10/1704]	Time 0.576 (0.740)	Data 3.46e-04 (4.67e-04)	Tok/s 39246 (38191)	LR 1.288e-05
2021-10-17 09:54:08 - INFO - 12 - TRAIN [0][20/1704]	Time 0.585 (0.707)	Data 5.40e-04 (4.89e-04)	Tok/s 38239 (37899)	LR 1.622e-05
2021-10-17 09:54:15 - INFO - 12 - TRAIN [0][30/1704]	Time 0.604 (0.707)	Data 5.23e-04 (4.89e-04)	Tok/s 37580 (37498)	LR 2.042e-05
2021-10-17 09:54:22 - INFO - 12 - TRAIN [0][40/1704]	Time 0.827 (0.692)	Data 5.92e-04 (4.89e-04)	Tok/s 38629 (37570)	LR 2.570e-05
2021-10-17 09:54:28 - INFO - 12 - TRAIN [0][50/1704]	Time 0.837 (0.680)	Data 9.06e-04 (4.94e-04)	Tok/s 37781 (37452)	LR 3.236e-05
2021-10-17 09:54:35 - INFO - 12 - TRAIN [0][60/1704]	Time 0.770 (0.682)	Data 3.29e-04 (4.92e-04)	Tok/s 41431 (37503)	LR 4.074e-05
2021-10-17 09:54:41 - INFO - 12 - TRAIN [0][70/1704]	Time 0.839 (0.680)	Data 5.35e-04 (4.84e-04)	Tok/s 37538 (37305)	LR 5.129e-05
2021-10-17 09:54:48 - INFO - 12 - TRAIN [0][80/1704]	Time 0.580 (0.680)	Data 5.47e-04 (4.73e-04)	Tok/s 39099 (37408)	LR 6.457e-05
